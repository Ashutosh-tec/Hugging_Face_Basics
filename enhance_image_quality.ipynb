{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle competetion link\n",
    "https://www.kaggle.com/t/3e046f08abcb44969613c750d7fe9243\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:12:45.316923Z",
     "iopub.status.busy": "2024-12-16T18:12:45.316616Z",
     "iopub.status.idle": "2024-12-16T18:12:53.700420Z",
     "shell.execute_reply": "2024-12-16T18:12:53.699477Z",
     "shell.execute_reply.started": "2024-12-16T18:12:45.316894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import join\n",
    "from torchvision.transforms import v2\n",
    "from torch import optim, nn\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:12:53.702576Z",
     "iopub.status.busy": "2024-12-16T18:12:53.702033Z",
     "iopub.status.idle": "2024-12-16T18:12:53.711562Z",
     "shell.execute_reply": "2024-12-16T18:12:53.710675Z",
     "shell.execute_reply.started": "2024-12-16T18:12:53.702535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageEnhancementDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder, input_transform=None, label_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (str): Path to the folder containing input images.\n",
    "            label_folder (str): Path to the folder containing label images.\n",
    "            input_transform (callable, optional): Transform to be applied to input images.\n",
    "            label_transform (callable, optional): Transform to be applied to label images.\n",
    "        \"\"\"\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.input_transform = input_transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "        # Ensure both folders have the same number of files\n",
    "        self.image_filenames = sorted(os.listdir(image_folder))\n",
    "        self.label_filenames = sorted(os.listdir(label_folder))\n",
    "\n",
    "        assert len(self.image_filenames) == len(self.label_filenames), \"Mismatch between image and label counts.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the input and label images\n",
    "        image_path = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        label_path = os.path.join(self.label_folder, self.label_filenames[idx])\n",
    "\n",
    "        image = Image.open(image_path) #.convert(\"RGB\")\n",
    "        label = Image.open(label_path) #.convert(\"RGB\")\n",
    "\n",
    "        # Ensure label size is 4x the input size\n",
    "        input_size = image.size  # (width, height)\n",
    "        label_size = label.size  # (width, height)\n",
    "        expected_label_size = (input_size[0] * 4, input_size[1] * 4)\n",
    "\n",
    "        assert label_size == expected_label_size, (\n",
    "            f\"Label size {label_size} does not match the expected size {expected_label_size} for input {input_size}.\"\n",
    "        )\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.input_transform:\n",
    "            image = self.input_transform(image)\n",
    "\n",
    "        if self.label_transform:\n",
    "            label = self.label_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:12:53.712809Z",
     "iopub.status.busy": "2024-12-16T18:12:53.712548Z",
     "iopub.status.idle": "2024-12-16T18:12:53.726340Z",
     "shell.execute_reply": "2024-12-16T18:12:53.725543Z",
     "shell.execute_reply.started": "2024-12-16T18:12:53.712785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"/kaggle/input/enhance-the-dark-world/archive\"\n",
    "train_dir = join(root_dir,\"train\")\n",
    "val_dir = join(root_dir,\"val\")\n",
    "test_dir = join(root_dir,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:12:53.728820Z",
     "iopub.status.busy": "2024-12-16T18:12:53.728502Z",
     "iopub.status.idle": "2024-12-16T18:12:53.783671Z",
     "shell.execute_reply": "2024-12-16T18:12:53.782863Z",
     "shell.execute_reply.started": "2024-12-16T18:12:53.728786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_transform = v2.Compose([\n",
    "        v2.ToImage(), \n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        transforms.Normalize(mean=[0.183, 0.198, 0.176], std=[0.083, 0.085, 0.081]),  # Normalize to a standard range\n",
    "])\n",
    "\n",
    "label_transform=  v2.Compose([v2.ToImage(),  v2.ToDtype(torch.float32, scale=True)])\n",
    "train_dataset = ImageEnhancementDataset(\n",
    "    join(train_dir,\"train\"), join(train_dir,\"gt\"),input_transform=image_transform,label_transform=label_transform\n",
    ")\n",
    "\n",
    "val_dataset = ImageEnhancementDataset(\n",
    "    join(val_dir,\"val\"), join(val_dir, \"gt\"), input_transform=image_transform, label_transform=label_transform\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, num_workers = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:12:53.785078Z",
     "iopub.status.busy": "2024-12-16T18:12:53.784824Z",
     "iopub.status.idle": "2024-12-16T18:13:16.536685Z",
     "shell.execute_reply": "2024-12-16T18:13:16.535890Z",
     "shell.execute_reply.started": "2024-12-16T18:12:53.785054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 3, 640, 1024])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Conv2d1x1(nn.Module):\n",
    "    def __init__(self, input_channels: int, reduction_factor: int = 1, out_channels: int = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if out_channels is None:\n",
    "            out_channels = input_channels // reduction_factor\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # define the 1x1 convolution layer\n",
    "        self.conv = nn.Conv2d(in_channels=input_channels, out_channels=out_channels, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class DepthwiseConv2d(nn.Module):\n",
    "    def __init__(self, input_channels: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        padding_size = kernel_size // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=input_channels, out_channels=input_channels,\n",
    "                              kernel_size=(kernel_size, kernel_size), groups=input_channels,\n",
    "                              padding=(padding_size, padding_size))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self.conv(input_tensor)\n",
    "\n",
    "\n",
    "class PointwiseConv2d(nn.Module):\n",
    "    def __init__(self, input_channels: int, out_channels: int = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if out_channels is None:\n",
    "            out_channels = input_channels\n",
    "\n",
    "        self.conv = Conv2d1x1(input_channels=input_channels, out_channels=out_channels)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self.conv(input_tensor)\n",
    "\n",
    "\n",
    "class TwoFoldAttentionModule(nn.Module):\n",
    "    class ChannelUnit(nn.Module):\n",
    "        def __init__(self, input_channels: int):\n",
    "            super().__init__()\n",
    "\n",
    "            self.in_channels = input_channels\n",
    "\n",
    "            # we define a global average pooling layer that extracts first-order statistics of features\n",
    "            self.global_avg_pooling = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "\n",
    "            # we then define two 1x1 convolutions that will work on half of the input channels and that will produce\n",
    "            # half of the output channels; these two 1x1 convolutions will not reduce the number of channels of the\n",
    "            # input tensor\n",
    "            conv_1x1_input_channels = input_channels // 2\n",
    "            self.conv1x1_1 = Conv2d1x1(input_channels=conv_1x1_input_channels)\n",
    "            self.conv1x1_2 = Conv2d1x1(input_channels=conv_1x1_input_channels)\n",
    "\n",
    "        def forward(self, input_tensor):\n",
    "            # first, we feed the input to the global average pooling layer to extract first-order statistics of features\n",
    "            # input_size = (N, in_channels, H, W)\n",
    "            first_order_statistics = self.global_avg_pooling(input_tensor)  # output_size = (N, in_channels, 1, 1)\n",
    "\n",
    "            # after producing first order statistic of features, we need to split the output tensor of the global\n",
    "            # average pooling into two tensors along the channel dimension\n",
    "            half_channels = self.in_channels // 2\n",
    "            first_half_input, second_half_input = torch.split(first_order_statistics,\n",
    "                                                              split_size_or_sections=half_channels, dim=1)\n",
    "            # output: two tensors of size (N, in_channels/2, 1, 1)\n",
    "\n",
    "            # now that we obtained the two halves of the channels, we feed them respectively to the first and second 1x1\n",
    "            # convolutions that will produce half the output channels\n",
    "            first_half_output = self.conv1x1_1(first_half_input)  # output_size = (N, in_channels/2, 1, 1)\n",
    "            second_half_output = self.conv1x1_2(second_half_input)  # output_size = (N, in_channels/2, 1, 1)\n",
    "\n",
    "            # we then concatenate the two halves of the output channels to get all the output channels\n",
    "            concatenated_halves = torch.cat((first_half_output, second_half_output), dim=1)\n",
    "            # output_size = (N, in_channels, 1, 1)\n",
    "\n",
    "            # now, we compute element-wise multiplication of the input tensor (x) and the output tensor produced by the\n",
    "            # concatenation operation\n",
    "            output = torch.mul(concatenated_halves, input_tensor)  # output_size = (N, in_channels, 1, 1)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class PositionalUnit(nn.Module):\n",
    "        def __init__(self, input_channels: int):\n",
    "            super().__init__()\n",
    "\n",
    "            # define the average pooling and the max pooling layers with a large kernel\n",
    "            self.avg_pooling = nn.AvgPool2d(kernel_size=(7, 7))\n",
    "            self.max_pooling = nn.MaxPool2d(kernel_size=(7, 7))\n",
    "\n",
    "            # define the final convolutional layer\n",
    "            kernel_size = 7\n",
    "            padding_size = kernel_size // 2\n",
    "            self.conv2d_1 = nn.Conv2d(in_channels=input_channels * 2, out_channels=input_channels,\n",
    "                                      kernel_size=(kernel_size, kernel_size),\n",
    "                                      padding=(padding_size, padding_size))\n",
    "\n",
    "        def forward(self, input_tensor):\n",
    "            # get the spatial dimensions of the input tensor\n",
    "            height = input_tensor.size()[2]\n",
    "            width = input_tensor.size()[3]\n",
    "\n",
    "            # first, we feed the input tensor to the average pooling layer and to the max pooling layer\n",
    "            output_max_pool = self.max_pooling(input_tensor)\n",
    "            output_avg_pool = self.avg_pooling(input_tensor)\n",
    "\n",
    "            # then, we concatenate the two outputs produced by the max\n",
    "            output_pool = torch.cat((output_max_pool, output_avg_pool), dim=1)\n",
    "\n",
    "            # now, we upsample the output concatenation to recover spatial dimensions\n",
    "            upsampled_out = F.interpolate(output_pool, size=(height, width), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            # once we upsampled the concatenation, we apply\n",
    "            output = self.conv2d_1(upsampled_out)\n",
    "\n",
    "            return output\n",
    "\n",
    "    def __init__(self, input_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # define first 1x1 convolution layer\n",
    "        self.conv1x1_1 = Conv2d1x1(input_channels=input_channels, reduction_factor=16)\n",
    "\n",
    "        # now, we define respectively the Channel Unit (CA Unit) and the Positional Unit (Pos Unit)\n",
    "        self.ca_unit = self.ChannelUnit(input_channels=self.conv1x1_1.out_channels)\n",
    "        self.pos_unit = self.PositionalUnit(input_channels=self.conv1x1_1.out_channels)\n",
    "\n",
    "        # define the last 1x1 convolution layer used to recover original channel dimension\n",
    "        self.conv1x1_2 = Conv2d1x1(input_channels=self.conv1x1_1.out_channels, out_channels=input_channels)\n",
    "\n",
    "        # define the sigmoid function to generate the final attention mast\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # first, we pass the input tensor to the first 1x1 convolution layer\n",
    "        conv_1x1_1_out = self.conv1x1_1(input_tensor)\n",
    "\n",
    "        # then, we pass the output produced by the 1x1 convolution layer to the Channel unit\n",
    "        ca_unit_out = self.ca_unit(conv_1x1_1_out)\n",
    "\n",
    "        # we pass the output produced by the 1x1 convolution layer to the Positional Unit\n",
    "        pos_unit_out = self.pos_unit(conv_1x1_1_out)\n",
    "\n",
    "        # we compute element-wise sum of the two tensors produced by the two units\n",
    "        sum_output = torch.add(ca_unit_out, pos_unit_out)\n",
    "\n",
    "        # we feed the aggregated tensors to the last 1x1 conv layer to recover channel dimensions\n",
    "        conv_1x1_2_out = self.conv1x1_2(sum_output)\n",
    "\n",
    "        # we feed the output of the 1x1 conv layer to the sigmoid layer to compute the final attention mast\n",
    "        sigmoid_out = self.sigmoid(conv_1x1_2_out)\n",
    "\n",
    "        # finally, we compute the element-wise multiplication between the computed final attention mask and the input\n",
    "        # tensor\n",
    "        output = torch.mul(input_tensor, sigmoid_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class AdaptiveResidualBlock(nn.Module):\n",
    "    class BottleneckPath(nn.Module):\n",
    "        def __init__(self, input_channels: int):\n",
    "            super().__init__()\n",
    "\n",
    "            # define the first depthwise convolutional layer with kernel size 3x3\n",
    "            self.dw_conv_1 = DepthwiseConv2d(input_channels=input_channels, kernel_size=3)\n",
    "\n",
    "            # define the first pointwise convolutional layer, followed by LeakyReLU\n",
    "            self.pw_conv_1 = PointwiseConv2d(input_channels=input_channels)\n",
    "            self.lrelu_1 = nn.LeakyReLU()\n",
    "\n",
    "            # define the second depthwise convolutional layer with kernel size 3x3\n",
    "            self.dw_conv_2 = DepthwiseConv2d(input_channels=input_channels, kernel_size=3)\n",
    "\n",
    "            # define the TFAM layer, followed by LeakyReLU\n",
    "            self.tfam = TwoFoldAttentionModule(input_channels=input_channels)\n",
    "            self.lrelu_2 = nn.LeakyReLU()\n",
    "\n",
    "            # define the second pointwise convolution layer\n",
    "            self.pw_conv_2 = PointwiseConv2d(input_channels=input_channels)\n",
    "\n",
    "        def forward(self, input_tensor):\n",
    "            # first, we feed the input tensor to the first depthwise convolutional layer\n",
    "            dw_conv_1_out = self.dw_conv_1(input_tensor)\n",
    "\n",
    "            # then, we feed the output to the first pointwise convolutional layer and to the first LReLU layer\n",
    "            pw_conv_1_out = self.pw_conv_1(dw_conv_1_out)\n",
    "            lrelu_1_out = self.lrelu_1(pw_conv_1_out)\n",
    "\n",
    "            # after this, we feed the output to the second depthwise convolutional layer\n",
    "            dw_conv_2_out = self.dw_conv_2(lrelu_1_out)\n",
    "\n",
    "            # then, we feed the output to the TFAM module and to the second LReLU\n",
    "            tfam_out = self.tfam(dw_conv_2_out)\n",
    "            lrelu_2_out = self.lrelu_2(tfam_out)\n",
    "\n",
    "            # finally, the output is fed to the second pointwise convolutional layer and returned\n",
    "            output = self.pw_conv_2(lrelu_2_out)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class AdaptivePath(nn.Module):\n",
    "        def __init__(self, input_channels: int):\n",
    "            super().__init__()\n",
    "\n",
    "            # define the global average pooling layer\n",
    "            self.global_avg_pooling = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "\n",
    "            # define the pointwise convolution layer\n",
    "            self.pw_conv = PointwiseConv2d(input_channels=input_channels)\n",
    "\n",
    "        def forward(self, input_tensor):\n",
    "            # first, we feed the input tensor to the global average pooling layer\n",
    "            global_avg_out = self.global_avg_pooling(input_tensor)\n",
    "\n",
    "            # finally, we feed the output to the pointwise convolution layer\n",
    "            output = self.pw_conv(global_avg_out)\n",
    "\n",
    "            return output\n",
    "\n",
    "    class ResidualPath(nn.Module):\n",
    "        def __init__(self, input_channels: int):\n",
    "            super().__init__()\n",
    "\n",
    "            # define the depthwise convolution layer with kernel size 3x3\n",
    "            self.dw_conv = DepthwiseConv2d(input_channels=input_channels, kernel_size=3)\n",
    "\n",
    "        def forward(self, input_tensor):\n",
    "            return self.dw_conv(input_tensor)\n",
    "\n",
    "    def __init__(self, input_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # first, we define the bottleneck path\n",
    "        self.bn_path = self.BottleneckPath(input_channels=input_channels)\n",
    "\n",
    "        # second, we define the adaptive path\n",
    "        self.ad_path = self.AdaptivePath(input_channels=input_channels)\n",
    "\n",
    "        # third, we define the residual path\n",
    "        self.res_path = self.ResidualPath(input_channels=input_channels)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # as first step, we pass the input tensor to the bottleneck path\n",
    "        bn_path_out = self.bn_path(input_tensor)\n",
    "\n",
    "        # then, we compute element-wise sum between the input tensor and the output of the bottleneck path\n",
    "        sum_bn_input = torch.add(input_tensor, bn_path_out)\n",
    "\n",
    "        # we feed the sum to the residual path\n",
    "        res_path_out = self.res_path(sum_bn_input)\n",
    "\n",
    "        # then, we pass the input tensor to the adaptive path\n",
    "        ad_path_out = self.ad_path(input_tensor)\n",
    "\n",
    "        # finally, we compute the output as the element-wise sum between the output of the residual path and the output\n",
    "        # of the adaptive path\n",
    "        output = torch.add(res_path_out, ad_path_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResidualConcatenationBlock(nn.Module):\n",
    "    def __init__(self, input_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # the definition of a Residual Concatenation Block contains three different Adaptive Residual Blocks that share\n",
    "        # the same weights; in order to implement this, we simply define one RCB that will be called three times in the\n",
    "        # forward function\n",
    "        self.arb = AdaptiveResidualBlock(input_channels=input_channels)\n",
    "\n",
    "        # define the first 1x1 convolutional layer\n",
    "        first_conv_input_channels = input_channels * 2\n",
    "        self.conv_1x1_1 = Conv2d1x1(input_channels=first_conv_input_channels, out_channels=input_channels)\n",
    "\n",
    "        # define the second 1x1 convolutional layer\n",
    "        second_conv_input_channels = input_channels * 3\n",
    "        self.conv_1x1_2 = Conv2d1x1(input_channels=second_conv_input_channels, out_channels=input_channels)\n",
    "\n",
    "        # define the third 1x1 convolutional layer\n",
    "        third_conv_input_channels = input_channels * 4\n",
    "        self.conv_1x1_3 = Conv2d1x1(input_channels=third_conv_input_channels, out_channels=input_channels)\n",
    "\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        # first, feed the input tensor to the ARB\n",
    "        arb_1_out = self.arb(input_tensor)\n",
    "\n",
    "        # second, we concatenate the output of the first ARB block with the input tensor\n",
    "        concat_1_out = torch.cat((input_tensor, arb_1_out), dim=1)\n",
    "\n",
    "        # after this, we feed the concatenation to the first 1x1 convolutional layer\n",
    "        conv_1x1_1_out = self.conv_1x1_1(concat_1_out)\n",
    "\n",
    "        # we feed the output of the 1x1 convolutional layer to the ARB\n",
    "        arb_2_out = self.arb(conv_1x1_1_out)\n",
    "\n",
    "        # we concatenate the output of the second ARB block with the previous concatenation\n",
    "        concat_2_out = torch.cat((concat_1_out, arb_2_out), dim=1)\n",
    "\n",
    "        # we feed the concatenation to the second 1x1 conv\n",
    "        conv_1x1_2_out = self.conv_1x1_2(concat_2_out)\n",
    "\n",
    "        # we feed the output of the second 1x1 conv layer to the ARB\n",
    "        arb_3_out = self.arb(conv_1x1_2_out)\n",
    "\n",
    "        # we concatenate the output of the third ARB block with the previous concatenation\n",
    "        concat_3_out = torch.cat((concat_2_out, arb_3_out), dim=1)\n",
    "\n",
    "        # finally, we feed the concatenation to the third  and last 1x1 conv layer\n",
    "        output = self.conv_1x1_3(concat_3_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResidualModule(nn.Module):\n",
    "    def __init__(self, input_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # define the first Residual Concatenation Block\n",
    "        self.rcb_1 = ResidualConcatenationBlock(input_channels=input_channels)\n",
    "\n",
    "        # define the first 1x1 convolutional layer\n",
    "        first_conv_input_channels = input_channels * 2\n",
    "        self.conv_1x1_1 = Conv2d1x1(input_channels=first_conv_input_channels, out_channels=input_channels)\n",
    "\n",
    "        # define the second Residual Concatenation Block\n",
    "        self.rcb_2 = ResidualConcatenationBlock(input_channels=input_channels)\n",
    "\n",
    "        # define the second 1x1 convolutional layer\n",
    "        second_conv_input_channels = input_channels * 3\n",
    "        self.conv_1x1_2 = Conv2d1x1(input_channels=second_conv_input_channels, out_channels=input_channels)\n",
    "\n",
    "        # define the third Residual Concatenation Block\n",
    "        self.rcb_3 = ResidualConcatenationBlock(input_channels=input_channels)\n",
    "\n",
    "        # define the third 1x1 convolutional layer\n",
    "        third_conv_input_channels = input_channels * 4\n",
    "        self.conv_1x1_3 = Conv2d1x1(input_channels=third_conv_input_channels, out_channels=input_channels)\n",
    "\n",
    "    def forward(self, h_sfe):\n",
    "        # first, feed the input tensor (h_sfe) to the first RCB block\n",
    "        rcb_1_out = self.rcb_1(h_sfe)\n",
    "\n",
    "        # second, we concatenate the output of the first RCB block with the input tensor (h_sfe)\n",
    "        concat_1_out = torch.cat((h_sfe, rcb_1_out), dim=1)\n",
    "\n",
    "        # after this, we feed the concatenation to the first 1x1 convolutional layer\n",
    "        conv_1x1_1_out = self.conv_1x1_1(concat_1_out)\n",
    "\n",
    "        # we feed the output of the 1x1 convolutional layer to the second RCB\n",
    "        rcb_2_out = self.rcb_2(conv_1x1_1_out)\n",
    "\n",
    "        # we concatenate the output of the second RCB block with the previous concatenation\n",
    "        concat_2_out = torch.cat((concat_1_out, rcb_2_out), dim=1)\n",
    "\n",
    "        # we feed the concatenation to the second 1x1 conv\n",
    "        conv_1x1_2_out = self.conv_1x1_2(concat_2_out)\n",
    "\n",
    "        # we feed the output of the second 1x1 conv layer to the third RCB\n",
    "        rcb_3_out = self.rcb_3(conv_1x1_2_out)\n",
    "\n",
    "        # we concatenate the output of the third ARB block with the previous concatenation\n",
    "        concat_3_out = torch.cat((concat_2_out, rcb_3_out), dim=1)\n",
    "\n",
    "        # finally, we feed the concatenation to the third  and last 1x1 conv layer\n",
    "        h_rm = self.conv_1x1_3(concat_3_out)\n",
    "\n",
    "        return h_rm\n",
    "\n",
    "\n",
    "class FeatureModule(nn.Module):\n",
    "    def __init__(self, input_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # define the first layer, which is a TFAM\n",
    "        self.tfam = TwoFoldAttentionModule(input_channels=input_channels)\n",
    "\n",
    "        # define the second layer, which is a 3x3 conv layer\n",
    "        kernel_size = 3\n",
    "        padding_size = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels=input_channels, out_channels=input_channels,\n",
    "                              kernel_size=(kernel_size, kernel_size), padding=padding_size)\n",
    "\n",
    "    def forward(self, h_rm, h_sfe):\n",
    "        # first, we feed the input tensor (h_rm) to the tfam layer\n",
    "        tfam_out = self.tfam(h_rm)\n",
    "\n",
    "        # then, we feed the output of the tfam layer to the convolutional layer\n",
    "        h_gfe = self.conv(tfam_out)\n",
    "\n",
    "        # finally, we compute the element-wise sum between the output of the convolutional layer and the shallow\n",
    "        # features\n",
    "        h_fm = torch.add(h_gfe, h_sfe)\n",
    "\n",
    "        return h_fm\n",
    "\n",
    "\n",
    "class UpNetModule(nn.Module):\n",
    "    class Upsample2x(nn.Module):\n",
    "        def __init__(self, input_channels: int):\n",
    "            super().__init__()\n",
    "\n",
    "            kernel_size = 3\n",
    "            padding_size = kernel_size // 2\n",
    "\n",
    "            # define the submodule that produces a feature map upsampled by 2x\n",
    "            self.conv = nn.Conv2d(in_channels=input_channels, out_channels=input_channels * 4, kernel_size=(3, 3),\n",
    "                                  padding=padding_size)\n",
    "            self.pix_shuf = nn.PixelShuffle(upscale_factor=2)\n",
    "\n",
    "        def forward(self, input_tensor):\n",
    "            # feed the input tensor to the conv layer\n",
    "            conv_out = self.conv(input_tensor)\n",
    "\n",
    "            # feed the output of the conv layer to the pixel shuffle layer\n",
    "            pix_shuf_out = self.pix_shuf(conv_out)\n",
    "\n",
    "            return pix_shuf_out\n",
    "\n",
    "    class Upsample3x(nn.Module):\n",
    "        def __init__(self, input_channels: int):\n",
    "            super().__init__()\n",
    "\n",
    "            kernel_size = 3\n",
    "            padding_size = kernel_size // 2\n",
    "\n",
    "            # define the submodule that produces a feature map upsampled by 3x\n",
    "            self.conv = nn.Conv2d(in_channels=input_channels, out_channels=input_channels * 9, kernel_size=(3, 3),\n",
    "                                  padding=padding_size)\n",
    "            self.pix_shuf = nn.PixelShuffle(upscale_factor=3)\n",
    "\n",
    "        def forward(self, input_tensor):\n",
    "            # feed the input tensor to the conv layer\n",
    "            conv_out = self.conv(input_tensor)\n",
    "\n",
    "            # feed the output of the conv layer to the pixel shuffle layer\n",
    "            pix_shuf_out = self.pix_shuf(conv_out)\n",
    "\n",
    "            return pix_shuf_out\n",
    "\n",
    "    class Upsample4x(nn.Module):\n",
    "        def __init__(self, input_channels: int):\n",
    "            super().__init__()\n",
    "\n",
    "            # define the first submodule that produces a feature map upsampled by 4x\n",
    "            self.upsample_4x = nn.Sequential(UpNetModule.Upsample2x(input_channels=input_channels),\n",
    "                                             UpNetModule.Upsample2x(input_channels=input_channels))\n",
    "\n",
    "        def forward(self, input_tensor):\n",
    "            # feed the input tensor to the upsampler\n",
    "            return self.upsample_4x(input_tensor)\n",
    "\n",
    "    def __init__(self, input_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # define the submodule that produces a feature map upsampled by 2x\n",
    "        self.upsample_2x = self.Upsample2x(input_channels=input_channels)\n",
    "\n",
    "        # define the submodule that produces a feature map upsampled by 3x\n",
    "        self.upsample_3x = self.Upsample3x(input_channels=input_channels)\n",
    "\n",
    "        # define the submodule that produces a feature map upsampled by 3x\n",
    "        self.upsample_4x = self.Upsample4x(input_channels=input_channels)\n",
    "\n",
    "    def forward(self, h_fm, scale: int):\n",
    "        # feed the input tensor to one of the upsamplers according to the given scale\n",
    "        if scale == 2:\n",
    "            upsampled = self.upsample_2x(h_fm)\n",
    "        elif scale == 3:\n",
    "            upsampled = self.upsample_3x(h_fm)\n",
    "        elif scale == 4:\n",
    "            upsampled = self.upsample_4x(h_fm)\n",
    "        else:\n",
    "            raise Exception(f\"Scale factor {scale} is invalid, select between 2, 3 or 4\")\n",
    "\n",
    "        return upsampled\n",
    "\n",
    "\n",
    "class MultiPathResidualNetwork(nn.Module):\n",
    "    def __init__(self, input_channels: int, n_features: int = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize initial shallow feature extractor\n",
    "        kernel_size = 3\n",
    "        padding_size = kernel_size // 2\n",
    "        self.sfe = nn.Conv2d(in_channels=input_channels, out_channels=n_features, kernel_size=(3, 3),\n",
    "                             padding=padding_size)\n",
    "\n",
    "        # define the Residual Module\n",
    "        self.rm = ResidualModule(input_channels=n_features)\n",
    "\n",
    "        # define the Feature Module\n",
    "        self.fm = FeatureModule(input_channels=n_features)\n",
    "\n",
    "        # define teh UpNet Module\n",
    "        self.upnet = UpNetModule(input_channels=n_features)\n",
    "\n",
    "        # define the final 3x3 convolution that restores the channels to three RGB channels\n",
    "        self.out_conv = nn.Conv2d(in_channels=n_features, out_channels=input_channels, kernel_size=(3, 3),\n",
    "                                  padding=padding_size)\n",
    "\n",
    "    def forward(self, lrs, scale: int):\n",
    "        # input is the batch of low resolution images, with shape (N, 3, 64, 64)\n",
    "        h_sfe = self.sfe(lrs)  # output size (N, 64, 64, 64)\n",
    "\n",
    "        # feed h_sfe to the residual module\n",
    "        h_rm = self.rm(h_sfe)  # output size (N, 64, 64, 64)\n",
    "\n",
    "        # feed h_rm and h_sfe to the feature module\n",
    "        h_fm = self.fm(h_rm, h_sfe)  # output size (N, 64, 64, 64)\n",
    "\n",
    "        # feed h_fm to the upnet module\n",
    "        upscaled_fm = self.upnet(h_fm, scale)  # output size (N, 64, 64 * scale, 64 * scale)\n",
    "\n",
    "        # feed upscaled feature map to the last 3x3 conv layer to get the final hr image in 3 RGB channels\n",
    "        srs = self.out_conv(upscaled_fm)  # output size (N, 3,  64 * scale, 64 * scale)\n",
    "\n",
    "        return srs\n",
    "\n",
    "# Model Test\n",
    "if __name__ == \"__main__\":\n",
    "    model = MultiPathResidualNetwork(input_channels=3, n_features=128)\n",
    "    x = torch.randn(4, 3, 160, 256)  # Input shape: [batch_size, channels, height, width]\n",
    "    y = model(x, scale = 4)\n",
    "    print(f\"Output shape: {y.shape}\")  # Expected output shape: [4, 3, 640, 1024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:13:16.538048Z",
     "iopub.status.busy": "2024-12-16T18:13:16.537685Z",
     "iopub.status.idle": "2024-12-16T18:13:17.470366Z",
     "shell.execute_reply": "2024-12-16T18:13:17.469439Z",
     "shell.execute_reply.started": "2024-12-16T18:13:16.538006Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 160, 256])\n",
      "Output shape: torch.Size([1, 3, 640, 1024])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import DataParallel\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = MultiPathResidualNetwork(input_channels=3, n_features=128).to(\"cuda\")\n",
    "\n",
    "model = DataParallel(model)\n",
    "\n",
    "\n",
    "# Test input\n",
    "# lrs = torch.randn(1, 3, 160,160).to(\"cuda\")  # Batch of 2 images, 3 channels, 64x64 resolution\n",
    "lrs = torch.randn(1,3,160,256)\n",
    "scale = 4\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    srs = model(lrs, scale)\n",
    "\n",
    "print(\"Input shape:\", lrs.shape)\n",
    "print(\"Output shape:\", srs.shape)\n",
    "assert srs.shape == (1, 3, 160 * scale, 256 * scale), \"Output shape is incorrect!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:13:17.471849Z",
     "iopub.status.busy": "2024-12-16T18:13:17.471551Z",
     "iopub.status.idle": "2024-12-16T18:13:17.477622Z",
     "shell.execute_reply": "2024-12-16T18:13:17.476799Z",
     "shell.execute_reply.started": "2024-12-16T18:13:17.471822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics.functional import structural_similarity_index_measure\n",
    "\n",
    "class SSIM_MSELoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8):\n",
    "        \"\"\"\n",
    "        Combines SSIM loss and MSE loss for image restoration tasks.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): Weight for MSE loss. SSIM weight is (1 - alpha).\n",
    "        \"\"\"\n",
    "        super(SSIM_MSELoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predicted (torch.Tensor): Predicted output image.\n",
    "            target (torch.Tensor): Ground truth image.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Combined loss value.\n",
    "        \"\"\"\n",
    "        mse = self.mse_loss(predicted, target)\n",
    "        ssim = structural_similarity_index_measure(predicted, target, data_range=1.0)  # SSIM assumes normalized data\n",
    "        combined_loss = self.alpha * mse + (1 - self.alpha) * (1 - ssim)\n",
    "        return combined_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:13:17.479397Z",
     "iopub.status.busy": "2024-12-16T18:13:17.479054Z",
     "iopub.status.idle": "2024-12-16T18:13:17.494319Z",
     "shell.execute_reply": "2024-12-16T18:13:17.493590Z",
     "shell.execute_reply.started": "2024-12-16T18:13:17.479361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, psnr_metric, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    First principles:\n",
    "    - Iterate through batches of training data\n",
    "    - Perform forward pass to get model predictions\n",
    "    - Calculate loss using the criterion\n",
    "    - Backpropagate and update model weights\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Super-resolution model\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        criterion (nn.Module): Loss function\n",
    "        psnr_metric (torchmetrics.Metric): PSNR metric\n",
    "        optimizer (torch.optim.Optimizer): Optimization algorithm\n",
    "        device (torch.device): Computing device (CPU/CUDA)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average training loss, average PSNR)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_psnr = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc='Training', unit='batch')\n",
    "    for lr_images, hr_images in progress_bar:\n",
    "        # Move data to the correct device\n",
    "        lr_images = lr_images.to(device)\n",
    "        hr_images = hr_images.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        sr_images = model(lrs=lr_images, scale=4)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(sr_images, hr_images)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate PSNR for monitoring\n",
    "        batch_psnr = psnr_metric(sr_images, hr_images).mean().item()\n",
    "        \n",
    "        # Update progress bar and tracking metrics\n",
    "        total_loss += loss.item()\n",
    "        total_psnr += batch_psnr\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}', \n",
    "            'PSNR': f'{batch_psnr:.2f}'\n",
    "        })\n",
    "    \n",
    "    # Return average loss and PSNR for the epoch\n",
    "    return total_loss / len(train_loader), total_psnr / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, psnr_metric, device):\n",
    "    \"\"\"\n",
    "    Validate the model on the validation dataset.\n",
    "    \n",
    "    First principles:\n",
    "    - Evaluate model performance on unseen data\n",
    "    - No gradient computation during validation\n",
    "    - Calculate loss and PSNR to assess model quality\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Super-resolution model\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        criterion (nn.Module): Loss function\n",
    "        psnr_metric (torchmetrics.Metric): PSNR metric\n",
    "        device (torch.device): Computing device (CPU/CUDA)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average validation loss, average PSNR)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_psnr = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(val_loader, desc='Validation', unit='batch')\n",
    "    with torch.no_grad():\n",
    "        for lr_images, hr_images in progress_bar:\n",
    "            # Move data to the correct device\n",
    "            lr_images = lr_images.to(device)\n",
    "            hr_images = hr_images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            sr_images = model(lrs=lr_images, scale=4)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(sr_images, hr_images)\n",
    "            \n",
    "            # Calculate PSNR for monitoring\n",
    "            batch_psnr = psnr_metric(sr_images, hr_images).mean().item()\n",
    "            \n",
    "            # Update tracking metrics\n",
    "            total_loss += loss.item()\n",
    "            total_psnr += batch_psnr\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}', \n",
    "                'PSNR': f'{batch_psnr:.2f}'\n",
    "            })\n",
    "    \n",
    "    # Return average loss and PSNR for validation\n",
    "    return total_loss / len(val_loader), total_psnr / len(val_loader)\n",
    "\n",
    "def train_super_resolution_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=3, \n",
    "    learning_rate=1e-4, \n",
    "    weight_decay=1e-5,\n",
    "    checkpoint_path=\"best_model.pth\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training loop for super-resolution model.\n",
    "    \n",
    "    First principles:\n",
    "    - Set up training environment\n",
    "    - Iterate through epochs\n",
    "    - Train and validate the model\n",
    "    - Track and save best performing model\n",
    "\n",
    "    Returns:\n",
    "        dict: Training history with losses and PSNR values\n",
    "    \"\"\"\n",
    "    # Ensure model is on CUDA\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function (typically L1 or L2 loss for image reconstruction)\n",
    "    criterion = SSIM_MSELoss(alpha=0.5)\n",
    "    \n",
    "    # PSNR metric\n",
    "    psnr_metric = PeakSignalNoiseRatio().to(device) # torchmetrics.image.PeakSignalNoiseRatio().to(device)\n",
    "    \n",
    "    # Optimizer with weight decay (L2 regularization)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (optional, but often helpful)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    # Track best model\n",
    "    best_val_psnr = float('-inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_psnr': [],\n",
    "        'val_loss': [],\n",
    "        'val_psnr': []\n",
    "    }\n",
    "    \n",
    "    # Start training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "        \n",
    "        # Train the model for one epoch\n",
    "        train_loss, train_psnr = train_one_epoch(\n",
    "            model, train_loader, criterion, psnr_metric, optimizer, device\n",
    "        )\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_psnr'].append(train_psnr)\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss, val_psnr = validate(model, val_loader, criterion, psnr_metric, device)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_psnr'].append(val_psnr)\n",
    "        \n",
    "        # Print the epoch stats\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Training PSNR: {train_psnr:.2f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation PSNR: {val_psnr:.2f}\")\n",
    "        \n",
    "        # Update learning rate scheduler based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save the best model (based on validation PSNR)\n",
    "        if val_psnr > best_val_psnr:\n",
    "            best_val_psnr = val_psnr\n",
    "            best_epoch = epoch\n",
    "            print(f\"New best model found, saving model at epoch {epoch+1}\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    print(f\"\\nTraining completed. Best validation PSNR: {best_val_psnr:.2f} at epoch {best_epoch+1}\")\n",
    "    torch.save(model.state_dict(), \"/kaggle/working/final_model.pth\")\n",
    "    # Return the training history\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T18:13:17.495663Z",
     "iopub.status.busy": "2024-12-16T18:13:17.495409Z",
     "iopub.status.idle": "2024-12-16T19:29:06.523484Z",
     "shell.execute_reply": "2024-12-16T19:29:06.522532Z",
     "shell.execute_reply.started": "2024-12-16T18:13:17.495640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/277 [00:00<?, ?batch/s]/opt/conda/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "Training: 100%|██████████| 277/277 [04:40<00:00,  1.01s/batch, Loss=0.0395, PSNR=30.10]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.13batch/s, Loss=0.0304, PSNR=26.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0831, Training PSNR: 25.76\n",
      "Validation Loss: 0.0379, Validation PSNR: 25.63\n",
      "New best model found, saving model at epoch 1\n",
      "\n",
      "Epoch [2/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0578, PSNR=27.95]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.12batch/s, Loss=0.0258, PSNR=27.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0513, Training PSNR: 27.42\n",
      "Validation Loss: 0.0334, Validation PSNR: 26.38\n",
      "New best model found, saving model at epoch 2\n",
      "\n",
      "Epoch [3/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0318, PSNR=31.41]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.12batch/s, Loss=0.0228, PSNR=28.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0482, Training PSNR: 28.37\n",
      "Validation Loss: 0.0305, Validation PSNR: 27.25\n",
      "New best model found, saving model at epoch 3\n",
      "\n",
      "Epoch [4/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0323, PSNR=29.43]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.12batch/s, Loss=0.0210, PSNR=28.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0425, Training PSNR: 29.43\n",
      "Validation Loss: 0.0285, Validation PSNR: 27.59\n",
      "New best model found, saving model at epoch 4\n",
      "\n",
      "Epoch [5/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0404, PSNR=29.56]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.11batch/s, Loss=0.0214, PSNR=28.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0414, Training PSNR: 29.61\n",
      "Validation Loss: 0.0290, Validation PSNR: 26.88\n",
      "\n",
      "Epoch [6/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0373, PSNR=28.92]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.12batch/s, Loss=0.0216, PSNR=29.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0413, Training PSNR: 29.51\n",
      "Validation Loss: 0.0290, Validation PSNR: 27.79\n",
      "New best model found, saving model at epoch 6\n",
      "\n",
      "Epoch [7/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0346, PSNR=31.18]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.12batch/s, Loss=0.0200, PSNR=29.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0407, Training PSNR: 29.78\n",
      "Validation Loss: 0.0273, Validation PSNR: 27.93\n",
      "New best model found, saving model at epoch 7\n",
      "\n",
      "Epoch [8/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0414, PSNR=29.78]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.12batch/s, Loss=0.0198, PSNR=29.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0403, Training PSNR: 29.83\n",
      "Validation Loss: 0.0272, Validation PSNR: 27.84\n",
      "\n",
      "Epoch [9/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0305, PSNR=31.93]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.11batch/s, Loss=0.0195, PSNR=29.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0399, Training PSNR: 29.87\n",
      "Validation Loss: 0.0268, Validation PSNR: 28.07\n",
      "New best model found, saving model at epoch 9\n",
      "\n",
      "Epoch [10/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0531, PSNR=28.22]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.11batch/s, Loss=0.0196, PSNR=29.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0397, Training PSNR: 29.94\n",
      "Validation Loss: 0.0268, Validation PSNR: 28.00\n",
      "\n",
      "Epoch [11/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0087, PSNR=36.17]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.11batch/s, Loss=0.0195, PSNR=29.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0396, Training PSNR: 29.95\n",
      "Validation Loss: 0.0267, Validation PSNR: 28.02\n",
      "\n",
      "Epoch [12/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0337, PSNR=30.29]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.12batch/s, Loss=0.0196, PSNR=29.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0394, Training PSNR: 29.98\n",
      "Validation Loss: 0.0268, Validation PSNR: 28.02\n",
      "\n",
      "Epoch [13/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0417, PSNR=30.20]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.12batch/s, Loss=0.0197, PSNR=29.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0394, Training PSNR: 30.02\n",
      "Validation Loss: 0.0269, Validation PSNR: 27.77\n",
      "\n",
      "Epoch [14/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0503, PSNR=29.30]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.10batch/s, Loss=0.0201, PSNR=29.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0394, Training PSNR: 29.97\n",
      "Validation Loss: 0.0274, Validation PSNR: 27.68\n",
      "\n",
      "Epoch [15/15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 277/277 [04:41<00:00,  1.02s/batch, Loss=0.0416, PSNR=29.92]\n",
      "Validation: 100%|██████████| 67/67 [00:21<00:00,  3.12batch/s, Loss=0.0192, PSNR=29.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0393, Training PSNR: 30.06\n",
      "Validation Loss: 0.0263, Validation PSNR: 28.19\n",
      "New best model found, saving model at epoch 15\n",
      "\n",
      "Training completed. Best validation PSNR: 28.19 at epoch 15\n"
     ]
    }
   ],
   "source": [
    "history = train_super_resolution_model(\n",
    "    model, train_loader, val_loader, num_epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# Create the model\n",
    "model = MultiPathResidualNetwork(input_channels=3, n_features=128).to(\"cuda\")\n",
    "\n",
    "model = DataParallel(model)\n",
    "\n",
    "# Step 2: Load the state dictionary\n",
    "checkpoint_path = \"/kaggle/working/best_model.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "def infer_super_res(image_path, model, transform=None, device=\"cpu\", plot=True):\n",
    "    \"\"\"\n",
    "    Perform super-resolution inference on a single image and plot the result.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        model (torch.nn.Module): Trained super-resolution model.\n",
    "        transform (callable, optional): Transformations to be applied to the input image.\n",
    "        device (str): Device to run the inference on (\"cpu\" or \"cuda\").\n",
    "    \"\"\"\n",
    "    # Load and preprocess the input image\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    if transform:\n",
    "        input_image = transform(input_image)\n",
    "    \n",
    "    # Add batch dimension and move to the device\n",
    "    input_image = input_image.unsqueeze(0).to(device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predicted_image = model(input_image, scale = 4).squeeze(0)  # Remove batch dimension\n",
    "    \n",
    "    # Convert the predicted tensor back to a NumPy array for plotting\n",
    "    predicted_image_np = predicted_image.permute(1, 2, 0).cpu().numpy()  # HWC format\n",
    "    predicted_image_np = (predicted_image_np * 255).astype(\"uint8\")  # Scale to [0, 255]\n",
    "\n",
    "    if plot == True:\n",
    "        # Plot the result\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(Image.open(image_path))  # Plot the original image\n",
    "        plt.title(\"Low-Light Image\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(predicted_image_np)  # Plot the predicted image\n",
    "        plt.title(\"Predicted High-Resolution Image\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        plt.show()\n",
    "    # Convert NumPy array to PIL image\n",
    "    predicted_image_pil = Image.fromarray(predicted_image_np)\n",
    "\n",
    "    return predicted_image_pil\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the input image\n",
    "    image_path = \"/kaggle/input/enhance-the-dark-world/archive/train/train/gt_00001.png\"\n",
    "\n",
    "\n",
    "    # Transform for the input image using torchvision v2\n",
    "    image_transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        transforms.Normalize(mean=[0.183, 0.198, 0.176], std=[0.083, 0.085, 0.081]),  # Normalize to a standard range\n",
    "    ])\n",
    "\n",
    "    # Perform inference and plot\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    infer_super_res(image_path, model, transform=image_transform, device=device)\n",
    "\n",
    "    i=0\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input/enhance-the-dark-world/archive/train/train'):\n",
    "        for filename in filenames:\n",
    "            image_path = os.path.join(dirname, filename)            \n",
    "            # Predict using the provided function\n",
    "            output_image = infer_super_res(image_path, model, transform=image_transform, device=device, plot=True)\n",
    "            i += 1\n",
    "            if i == 5:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T04:25:09.950231Z",
     "iopub.status.busy": "2024-12-16T04:25:09.949558Z",
     "iopub.status.idle": "2024-12-16T04:25:27.669351Z",
     "shell.execute_reply": "2024-12-16T04:25:27.668110Z",
     "shell.execute_reply.started": "2024-12-16T04:25:09.950174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "out_folder = \"/kaggle/working/output_images\"\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "create_directory(out_folder)\n",
    "\n",
    "# Transform for the input image using torchvision v2\n",
    "image_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.183, 0.198, 0.176], std=[0.083, 0.085, 0.081]),  # Normalize to a standard range\n",
    "])\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/enhance-the-dark-world/archive/test'):\n",
    "    for filename in filenames:\n",
    "        image_path = os.path.join(dirname, filename)\n",
    "        \n",
    "        # Predict using the provided function\n",
    "        output_image = infer_super_res(image_path, model, transform=image_transform, device=device, plot=False)\n",
    "        \n",
    "        # Save the output as a PNG file\n",
    "        img_name = os.path.join(out_folder, os.path.splitext(filename)[0] + \".png\")\n",
    "        \n",
    "        # Option 1: Save the PIL image directly\n",
    "        output_image.save(img_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T04:25:27.671173Z",
     "iopub.status.busy": "2024-12-16T04:25:27.670890Z",
     "iopub.status.idle": "2024-12-16T04:26:08.591367Z",
     "shell.execute_reply": "2024-12-16T04:26:08.590449Z",
     "shell.execute_reply.started": "2024-12-16T04:25:27.671146Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved to /kaggle/working/21F1001709.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def images_to_csv(folder_path, output_csv):\n",
    "    data_rows = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = Image.open(image_path).convert('L') \n",
    "            image_array = np.array(image).flatten()[::8]\n",
    "            # Replace 'test_' with 'gt_' in the ID\n",
    "            image_id = filename.split('.')[0].replace('test_', 'gt_')\n",
    "            data_rows.append([image_id, *image_array])\n",
    "    column_names = ['ID'] + [f'pixel_{i}' for i in range(len(data_rows[0]) - 1)]\n",
    "    df = pd.DataFrame(data_rows, columns=column_names)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f'Successfully saved to {output_csv}')\n",
    "\n",
    "images_to_csv(\"/kaggle/working/output_images\", \"/kaggle/working/21F1001709.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10486094,
     "sourceId": 89760,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
